{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 Pytorch Object Detection Exercises\n",
    "\n",
    "The goal of this collection is to offer a quick reference for both old and new users who are designing and prototyping object detection algorithms with pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "\n",
    "def show_boxes(lims=(0,6), **boxes):\n",
    "    \"\"\" Helper function to plot gt and anchors. \n",
    "    \"\"\"\n",
    "    gt_boxes = boxes['gt'] if 'gt' in boxes else []\n",
    "    anchor_boxes = boxes['anchor'] if 'anchor' in boxes else []\n",
    "    \n",
    "    plt.figure()\n",
    "    currentAxis = plt.gca()\n",
    "    for box in gt_boxes:\n",
    "        currentAxis.add_patch(Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=None, alpha=1, color='g'))\n",
    "\n",
    "    for anchor in anchor_boxes:\n",
    "        currentAxis.add_patch(Rectangle((anchor[0], anchor[1]), anchor[2]-anchor[0], anchor[3]-anchor[1], fill=None, alpha=1, color='r'))\n",
    "\n",
    "    plt.ylim(lims)\n",
    "    plt.xlim(lims)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate coordinates of anchor boxes\n",
    "```python\n",
    "base_size = 16 # pixel size in original image, also named stride or downsampling factor. \n",
    "scale_ls = [8, 16, 32] # under the same aspect ratio, scale = w_a/w_b or h_a/h_b\n",
    "ratio_ls = [0.5, 1, 2] # under the same scale, ratio = height/width\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code reference: https://github.com/NVIDIA/retinanet-examples/blob/c1ba8e7a9ffa036a387387426682e1a7b58ec707/retinanet/box.py#L5\n",
    "scale_ls = [8,16,32]\n",
    "ratio_ls = [0.5,1,2] \n",
    "base_size = 16\n",
    "def get_base_anchors(scale_ls, ratio_ls, base_size):\n",
    "    \"\"\" Get the base anchors over one cell.\n",
    "    \"\"\"\n",
    "    scales = torch.FloatTensor(scale_ls).repeat(len(ratio_ls), 1) # shape: (len(ratio_ls), len(scale_ls)), (3,3)\n",
    "    scales = scales.transpose(0, 1).contiguous().view(-1, 1) # shape: (len(ratio_ls)*len(scale_ls), 1), (9,1)\n",
    "    ratios = torch.FloatTensor(ratio_ls).repeat(len(scale_ls)) # shape: (len(ratio_ls)*len(scale_ls),), (9,)\n",
    "    \n",
    "    base_wh = torch.FloatTensor([base_size]).repeat(len(ratios), 2) # base width and height, shape: (9,2)\n",
    "    anchor_w = torch.round(torch.sqrt(base_wh[:, 0] * base_wh[:, 1] / ratios)) # anchor width, shape: (9,)\n",
    "    anchor_h = torch.round(anchor_w * ratios) # anchor height, shape: (9,)\n",
    "    anchor_wh = torch.stack((anchor_w, torch.round(anchor_w * ratios)), dim=1) # anchor width and height, shape: (9,2)\n",
    "    \n",
    "    # calculate the relative center coordinates, then the diff is the xmin,ymin and xmax,ymax\n",
    "    xy_min = 0.5*(base_wh-1) - 0.5*(anchor_wh* scales-1)\n",
    "    xy_max = 0.5*(base_wh-1) + 0.5*(anchor_wh* scales-1)\n",
    "    anchor_coord = torch.cat([xy_min, xy_max], dim=1)\n",
    "    \n",
    "    return anchor_coord\n",
    "\n",
    "get_base_anchors(scale_ls, ratio_ls, base_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate all anchors over the feature map\n",
    "\n",
    "```python\n",
    "feat_size = 28 # image size = feat_size * stride\n",
    "stride = 16\n",
    "scale_ls = [8,16,32]\n",
    "ratio_ls = [0.5,1,2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_ls = [8,16,32]\n",
    "ratio_ls = [0.5,1,2] \n",
    "stride = 16\n",
    "feat_size = 28\n",
    "\n",
    "base_anchors = get_base_anchors(scale_ls, ratio_ls, stride)\n",
    "\n",
    "def get_all_anchors(base_anchors, feat_size, stride):\n",
    "    \"\"\" Get all anchors over the entire feature map. \n",
    "    \n",
    "    Anchor coordinates could be outside the image.\n",
    "    \"\"\"\n",
    "    shift_x, shift_y = torch.meshgrid(torch.arange(0, feat_size*stride, stride), torch.arange(0, feat_size*stride, stride))\n",
    "    shift_xyxy = torch.stack((shift_x,shift_y,shift_x,shift_y),dim=2).to(dtype=torch.float32) # shape: (feat_size,feat_size,4)\n",
    "    base_anchors = base_anchors.view(-1,1,1,4)\n",
    "    all_anchors = (shift_xyxy+base_anchors).contiguous().view(-1,4)\n",
    "    \n",
    "    return all_anchors\n",
    "\n",
    "get_all_anchors(base_anchors, feat_size, stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Sort the anchors based on the IoU with groud truth boxes\n",
    "```python\n",
    "boxes = [[0.5,0.5,1.5,1.5],[2.5,2.5,5.,5.]]\n",
    "anchors = [[0.2,1.,2.,2.],[1.2,1.7,3.,3.6],[2.2,1.8,4.5,4.3]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = [[0.5,0.5,1.5,1.5],[2.5,2.5,5.,5.]]\n",
    "anchors = [[0.2,1.,2.,2.],[1.2,1.7,3.,3.6],[2.2,1.8,4.5,4.3]]\n",
    "\n",
    "show_boxes(gt=boxes, anchor=anchors)\n",
    "\n",
    "def sort_anchor_with_iou(boxes, anchors):\n",
    "    \"\"\" Sort anchors based on iou with gt boxes.\n",
    "    \"\"\"\n",
    "    boxes = torch.Tensor(boxes)\n",
    "    anchors = torch.Tensor(anchors)\n",
    "    xy1 = torch.max(anchors[:, None, :2], boxes[:, :2]) # broadcasting\n",
    "    xy2 = torch.min(anchors[:, None, 2:], boxes[:, 2:])\n",
    "    inter = torch.prod((xy2-xy1).clamp(0), dim=-1)\n",
    "    boxes_area = torch.prod(boxes[:, 2:] - boxes[:, :2], dim=-1)\n",
    "    anchors_area = torch.prod(anchors[:, 2:] - anchors[:, :2], dim=-1)\n",
    "    iou = inter / ((boxes_area[:, None]+anchors_area).view(inter.size()) - inter)\n",
    "    iou, indices = iou.max(-1) # iou shape: (num_anchors,), the elements in indices are index of gt box.\n",
    "    return iou, indices\n",
    "\n",
    "sort_anchor_with_iou(boxes, anchors) # [0.2174, 0.0733, 0.4286], [0, 1, 1], it means that the first anchor has the max iou 0.2174 with 0-th gt box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Convert class label to one hot\n",
    "```python\n",
    "cls_num = 8\n",
    "batch_labels = [0, 4, 1, 7]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_num = 8\n",
    "batch_labels = [0, 4, 1, 7]\n",
    "\n",
    "def onehot_encode(cls_num, labels):\n",
    "    \"\"\" Encode batch labels to one hot representation. \n",
    "    \"\"\"\n",
    "    labels = torch.LongTensor(labels).reshape(-1,1)\n",
    "    onehot = torch.zeros(labels.size()[0], cls_num).scatter_(1, labels, 1)\n",
    "    return onehot\n",
    "\n",
    "onehot_encode(cls_num, batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Given anchors, convert GT boxes to deltas and convert deltas to pred boxes.\n",
    "```python\n",
    "boxes = [2.5,2.5,5.,5.] # (x1,y1,x2,y2)\n",
    "anchors = [2.2,1.8,4.5,4.3]\n",
    "deltas = [[0.5, 0.1, 0.002, 0.0001]] # (d_x,d_y,d_w,d_h)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = [[2.5,2.5,5.,5.]]\n",
    "anchors = [[2.2,1.8,4.5,4.3]]\n",
    "deltas = [[0.5, 0.1, 0.002, 0.0001]]\n",
    "\n",
    "show_boxes(gt=boxes, anchor=anchors)\n",
    "\n",
    "def gt2delta(boxes, anchors):\n",
    "    \"\"\" Convert gt boxes to deltas (center shift and wh ratio in log scale) given anchors. \n",
    "    \"\"\"\n",
    "    anchors = torch.Tensor(anchors)\n",
    "    boxes = torch.Tensor(boxes)\n",
    "\n",
    "    anchors_wh = anchors[:, 2:] - anchors[:, :2]\n",
    "    anchors_ctr = anchors[:, :2] + anchors_wh * 0.5\n",
    "    boxes_wh = boxes[:, 2:] - boxes[:, :2]\n",
    "    boxes_ctr = boxes[:, :2] + boxes_wh * 0.5\n",
    "    return torch.cat(((boxes_ctr - anchors_ctr) / anchors_wh, torch.log(boxes_wh / anchors_wh)), dim=-1)\n",
    "\n",
    "\n",
    "def delta2pred(deltas, anchors):\n",
    "    \"\"\" Convert deltas to pred boxes given anchors. \n",
    "    \"\"\"\n",
    "    anchors = torch.Tensor(anchors)\n",
    "    deltas = torch.Tensor(deltas)\n",
    "\n",
    "    anchors_wh = anchors[:, 2:] - anchors[:, :2]\n",
    "    anchors_ctr = anchors[:, :2] + anchors_wh * 0.5\n",
    "    pred_ctr = deltas[:, :2] + anchors_ctr\n",
    "    pred_wh = torch.exp(deltas[:, 2:]) * anchors_wh\n",
    "    pred_boxes = torch.cat((pred_ctr - 0.5 * pred_wh, pred_ctr + 0.5 * pred_wh), dim=-1)\n",
    "    \n",
    "    return pred_boxes\n",
    "\n",
    "print(gt2delta(boxes, anchors))\n",
    "\n",
    "pred_boxes = delta2pred(deltas, anchors)\n",
    "show_boxes(gt=pred_boxes, anchor=anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. nms\n",
    "```python\n",
    "scores = [0.6, 0.9, 0.3, 0, 0.7, 0.4, 0.6, 0.4]\n",
    "boxes = [[3,4,4.5,5.6001], [2.9,4.1,4.6,5.8002], [2.95,3.8,4.6,5.5003], [3.3,4.9,4.4,6.1004], [5.4,4.2,6.5,6.2005], [5.3,4.5,6.7,6.8006], [5.3,1.7,6.5,3.7007], [5.4,1.8,6.6,3.5008]]\n",
    "classes = [0,0,0,0,0,0,1,1]\n",
    "nms_thres = 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [0.6, 0.9, 0.3, 0, 0.7, 0.4, 0.6, 0.4]\n",
    "boxes = [[3,4,4.5,5.6001], [2.9,4.1,4.6,5.8002], [2.95,3.8,4.6,5.5003], [3.3,4.9,4.4,6.1004], [5.4,4.2,6.5,6.2005], [5.3,4.5,6.7,6.8006], [5.3,1.7,6.5,3.7007], [5.4,1.8,6.6,3.5008]]\n",
    "classes = [0,0,0,0,0,0,1,1]\n",
    "nms_thres = 0.5\n",
    "\n",
    "show_boxes(lims=(1,7), gt=boxes[:6], anchor=boxes[6:]) # Just use gt and anchor to distinguish different classes. \n",
    "\n",
    "def nms(scores, boxes, classes, nms_thres):\n",
    "    \"\"\" Apply nms on input boxes with multiple classes. \n",
    "    \"\"\"\n",
    "    scores = torch.Tensor(scores)\n",
    "    boxes = torch.Tensor(boxes)\n",
    "    classes = torch.Tensor(classes)\n",
    "\n",
    "    # filter out 0 score boxes\n",
    "    non_idx = (scores.view(-1) > 0).nonzero()\n",
    "    scores, boxes, classes = scores[non_idx].view(-1), boxes[non_idx].view(-1, 4), classes[non_idx].view(-1)\n",
    "    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) # area of each box. \n",
    "\n",
    "    # sort boxes according to the scores\n",
    "    _, order = torch.sort(scores, dim=0, descending=True)\n",
    "\n",
    "    keep = []\n",
    "    while order.numel():\n",
    "        if order.numel() == 1:\n",
    "            keep.append(order.item())\n",
    "            break\n",
    "        else:\n",
    "            i = order[0].item()\n",
    "            keep.append(i)     # append the first box. \n",
    "\n",
    "            xy1 = torch.max(boxes[order[1:], :2], boxes[i, :2])\n",
    "            xy2 = torch.min(boxes[order[1:], 2:], boxes[i, 2:])\n",
    "            inter = torch.prod((xy2-xy1).clamp(0), dim=-1)\n",
    "\n",
    "            # boxes that should be saved:\n",
    "            # 1) the boxes iou with current box is less_equal thres, 2) not same class\n",
    "            criterion = ((inter / (areas[order[1:]] + areas[i] - inter) <= nms_thres) | (classes[order[1:]] != classes[i])).nonzero() \n",
    "            if criterion.numel() == 0:\n",
    "                break\n",
    "            order = order[criterion+1].view(-1) # criterion is based on order[1:], so need to add 1 to get new order. \n",
    "    return scores[keep], boxes[keep], classes[keep]\n",
    "\n",
    "_, keep_boxes, keep_classes = nms(scores, boxes, classes, nms_thres)\n",
    "show_boxes(lims=(1,7), gt=keep_boxes[keep_classes < 1], anchor=keep_boxes[keep_classes == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
