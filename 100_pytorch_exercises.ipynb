{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 Pytorch Object Detection Exercises\n",
    "\n",
    "The goal of this collection is to offer a quick reference for both old and new users who are designing and prototyping object detection algorithms with pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate coordinates of anchor boxes\n",
    "```python\n",
    "base_size = 16 # pixel size in original image, also named stride or downsampling factor. \n",
    "scale_ls = [8, 16, 32] # under the same aspect ratio, scale = w_a/w_b or h_a/h_b\n",
    "ratio_ls = [0.5, 1, 2] # under the same scale, ratio = height/width\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -84.,  -40.,   99.,   55.],\n",
       "        [ -56.,  -56.,   71.,   71.],\n",
       "        [ -36.,  -80.,   51.,   95.],\n",
       "        [-176.,  -88.,  191.,  103.],\n",
       "        [-120., -120.,  135.,  135.],\n",
       "        [ -80., -168.,   95.,  183.],\n",
       "        [-360., -184.,  375.,  199.],\n",
       "        [-248., -248.,  263.,  263.],\n",
       "        [-168., -344.,  183.,  359.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code reference: https://github.com/NVIDIA/retinanet-examples/blob/c1ba8e7a9ffa036a387387426682e1a7b58ec707/retinanet/box.py#L5\n",
    "scale_ls = [8,16,32]\n",
    "ratio_ls = [0.5,1,2] \n",
    "base_size = 16\n",
    "def get_base_anchors(scale_ls, ratio_ls, base_size):\n",
    "    \"\"\" Get the base anchors over one cell.\n",
    "    \"\"\"\n",
    "    scales = torch.FloatTensor(scale_ls).repeat(len(ratio_ls), 1) # shape: (len(ratio_ls), len(scale_ls)), (3,3)\n",
    "    scales = scales.transpose(0, 1).contiguous().view(-1, 1) # shape: (len(ratio_ls)*len(scale_ls), 1), (9,1)\n",
    "    ratios = torch.FloatTensor(ratio_ls).repeat(len(scale_ls)) # shape: (len(ratio_ls)*len(scale_ls),), (9,)\n",
    "    \n",
    "    base_wh = torch.FloatTensor([base_size]).repeat(len(ratios), 2) # base width and height, shape: (9,2)\n",
    "    anchor_w = torch.round(torch.sqrt(base_wh[:, 0] * base_wh[:, 1] / ratios)) # anchor width, shape: (9,)\n",
    "    anchor_h = torch.round(anchor_w * ratios) # anchor height, shape: (9,)\n",
    "    anchor_wh = torch.stack((anchor_w, torch.round(anchor_w * ratios)), dim=1) # anchor width and height, shape: (9,2)\n",
    "    \n",
    "    # calculate the relative center coordinates, then the diff is the xmin,ymin and xmax,ymax\n",
    "    xy_min = 0.5 * (base_wh - anchor_wh * scales) # 0.5*(base_wh-1) - 0.5*(anchor_wh-1)\n",
    "    xy_max = 0.5 * (base_wh + anchor_wh * scales) - 1 # 0.5*(base_wh-1) + 0.5*(anchor_wh-1)\n",
    "    anchor_coord = torch.cat([xy_min, xy_max], dim=1)\n",
    "    \n",
    "    return anchor_coord\n",
    "\n",
    "get_base_anchors(scale_ls, ratio_ls, base_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate all anchors over the feature map\n",
    "\n",
    "```python\n",
    "feat_size = 28 # image size = feat_size * stride\n",
    "stride = 16\n",
    "scale_ls = [8,16,32]\n",
    "ratio_ls = [0.5,1,2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-84., -40.,  99.,  55.],\n",
       "        [-84., -24.,  99.,  71.],\n",
       "        [-84.,  -8.,  99.,  87.],\n",
       "        ...,\n",
       "        [264.,  56., 615., 759.],\n",
       "        [264.,  72., 615., 775.],\n",
       "        [264.,  88., 615., 791.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_ls = [8,16,32]\n",
    "ratio_ls = [0.5,1,2] \n",
    "stride = 16\n",
    "feat_size = 28\n",
    "\n",
    "base_anchors = get_base_anchors(scale_ls, ratio_ls, stride)\n",
    "\n",
    "def get_all_anchors(base_anchors, feat_size, stride):\n",
    "    \"\"\" Get all anchors over the entire feature map. \n",
    "    \n",
    "    Anchor coordinates could be outside the image.\n",
    "    \"\"\"\n",
    "    shift_x, shift_y = torch.meshgrid(torch.arange(0, feat_size*stride, stride), torch.arange(0, feat_size*stride, stride))\n",
    "    shift_xyxy = torch.stack((shift_x,shift_y,shift_x,shift_y),dim=2).to(dtype=torch.float32) # shape: (28,28,4)\n",
    "    base_anchors = base_anchors.view(-1,1,1,4)\n",
    "    all_anchors = (shift_xyxy+base_anchors).contiguous().view(-1,4)\n",
    "    \n",
    "    return all_anchors\n",
    "\n",
    "get_all_anchors(base_anchors, feat_size, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
