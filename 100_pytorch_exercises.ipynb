{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 Pytorch Object Detection Exercises\n",
    "\n",
    "The goal of this collection is to offer a quick reference for both old and new users who are designing and prototyping object detection algorithms with pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate coordinates of anchor boxes\n",
    "```python\n",
    "base_size = 16 # pixel size in original image, also named stride or downsampling factor. \n",
    "scale_ls = [8, 16, 32] # under the same aspect ratio, scale = w_a/w_b or h_a/h_b\n",
    "ratio_ls = [0.5, 1, 2] # under the same scale, ratio = height/width\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -84.,  -40.,   99.,   55.],\n",
       "        [ -56.,  -56.,   71.,   71.],\n",
       "        [ -36.,  -80.,   51.,   95.],\n",
       "        [-176.,  -88.,  191.,  103.],\n",
       "        [-120., -120.,  135.,  135.],\n",
       "        [ -80., -168.,   95.,  183.],\n",
       "        [-360., -184.,  375.,  199.],\n",
       "        [-248., -248.,  263.,  263.],\n",
       "        [-168., -344.,  183.,  359.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code reference: https://github.com/NVIDIA/retinanet-examples/blob/c1ba8e7a9ffa036a387387426682e1a7b58ec707/retinanet/box.py#L5\n",
    "scale_ls = [8,16,32]\n",
    "ratio_ls = [0.5,1,2] \n",
    "base_size = 16\n",
    "def get_base_anchors(scale_ls, ratio_ls, base_size):\n",
    "    \"\"\" Get the base anchors over one cell.\n",
    "    \"\"\"\n",
    "    scales = torch.FloatTensor(scale_ls).repeat(len(ratio_ls), 1) # shape: (len(ratio_ls), len(scale_ls)), (3,3)\n",
    "    scales = scales.transpose(0, 1).contiguous().view(-1, 1) # shape: (len(ratio_ls)*len(scale_ls), 1), (9,1)\n",
    "    ratios = torch.FloatTensor(ratio_ls).repeat(len(scale_ls)) # shape: (len(ratio_ls)*len(scale_ls),), (9,)\n",
    "    \n",
    "    base_wh = torch.FloatTensor([base_size]).repeat(len(ratios), 2) # base width and height, shape: (9,2)\n",
    "    anchor_w = torch.round(torch.sqrt(base_wh[:, 0] * base_wh[:, 1] / ratios)) # anchor width, shape: (9,)\n",
    "    anchor_h = torch.round(anchor_w * ratios) # anchor height, shape: (9,)\n",
    "    anchor_wh = torch.stack((anchor_w, torch.round(anchor_w * ratios)), dim=1) # anchor width and height, shape: (9,2)\n",
    "    \n",
    "    # calculate the relative center coordinates, then the diff is the xmin,ymin and xmax,ymax\n",
    "    xy_min = 0.5*(base_wh-1) - 0.5*(anchor_wh* scales-1)\n",
    "    xy_max = 0.5*(base_wh-1) + 0.5*(anchor_wh* scales-1)\n",
    "    anchor_coord = torch.cat([xy_min, xy_max], dim=1)\n",
    "    \n",
    "    return anchor_coord\n",
    "\n",
    "get_base_anchors(scale_ls, ratio_ls, base_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate all anchors over the feature map\n",
    "\n",
    "```python\n",
    "feat_size = 28 # image size = feat_size * stride\n",
    "stride = 16\n",
    "scale_ls = [8,16,32]\n",
    "ratio_ls = [0.5,1,2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-84., -40.,  99.,  55.],\n",
       "        [-84., -24.,  99.,  71.],\n",
       "        [-84.,  -8.,  99.,  87.],\n",
       "        ...,\n",
       "        [264.,  56., 615., 759.],\n",
       "        [264.,  72., 615., 775.],\n",
       "        [264.,  88., 615., 791.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_ls = [8,16,32]\n",
    "ratio_ls = [0.5,1,2] \n",
    "stride = 16\n",
    "feat_size = 28\n",
    "\n",
    "base_anchors = get_base_anchors(scale_ls, ratio_ls, stride)\n",
    "\n",
    "def get_all_anchors(base_anchors, feat_size, stride):\n",
    "    \"\"\" Get all anchors over the entire feature map. \n",
    "    \n",
    "    Anchor coordinates could be outside the image.\n",
    "    \"\"\"\n",
    "    shift_x, shift_y = torch.meshgrid(torch.arange(0, feat_size*stride, stride), torch.arange(0, feat_size*stride, stride))\n",
    "    shift_xyxy = torch.stack((shift_x,shift_y,shift_x,shift_y),dim=2).to(dtype=torch.float32) # shape: (feat_size,feat_size,4)\n",
    "    base_anchors = base_anchors.view(-1,1,1,4)\n",
    "    all_anchors = (shift_xyxy+base_anchors).contiguous().view(-1,4)\n",
    "    \n",
    "    return all_anchors\n",
    "\n",
    "get_all_anchors(base_anchors, feat_size, stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Sort the anchors based on the IoU with groud truth boxes\n",
    "```python\n",
    "boxes = [[0.5,0.5,1.5,1.5],[2.5,2.5,5.,5.]]\n",
    "anchors = [[0.2,1.,2.,2.],[1.2,1.7,3.,3.6],[2.2,1.8,4.5,4.3]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.2174, 0.0733, 0.4286]), tensor([0, 1, 1]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = [[0.5,0.5,1.5,1.5],[2.5,2.5,5.,5.]]\n",
    "anchors = [[0.2,1.,2.,2.],[1.2,1.7,3.,3.6],[2.2,1.8,4.5,4.3]]\n",
    "lims = (0, 6)\n",
    "plt.figure()\n",
    "currentAxis = plt.gca()\n",
    "for box in boxes:\n",
    "    currentAxis.add_patch(Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=None, alpha=1, color='g'))\n",
    "\n",
    "for anchor in anchors:\n",
    "    currentAxis.add_patch(Rectangle((anchor[0], anchor[1]), anchor[2]-anchor[0], anchor[3]-anchor[1], fill=None, alpha=1, color='r'))\n",
    "    \n",
    "plt.ylim(lims)\n",
    "plt.xlim(lims)\n",
    "plt.show()\n",
    "\n",
    "def sort_anchor_with_iou(boxes, anchors):\n",
    "    \"\"\" Sort anchors based on iou with gt boxes.\n",
    "    \"\"\"\n",
    "    boxes = torch.Tensor(boxes)\n",
    "    anchors = torch.Tensor(anchors)\n",
    "    xy1 = torch.max(anchors[:, None, :2], boxes[:, :2]) # broadcasting\n",
    "    xy2 = torch.min(anchors[:, None, 2:], boxes[:, 2:])\n",
    "    inter = torch.prod((xy2-xy1).clamp(0), dim=-1)\n",
    "    boxes_area = torch.prod(boxes[:, 2:] - boxes[:, :2], dim=-1)\n",
    "    anchors_area = torch.prod(anchors[:, 2:] - anchors[:, :2], dim=-1)\n",
    "    iou = inter / ((boxes_area[:, None]+anchors_area).view(inter.size()) - inter)\n",
    "    iou, indices = iou.max(-1) # iou shape: (num_anchors,), the elements in indices are index of gt box.\n",
    "    return iou, indices\n",
    "\n",
    "sort_anchor_with_iou(boxes, anchors) # [0.2174, 0.0733, 0.4286], [0, 1, 1], it means that the first anchor has the max iou 0.2174 with 0-th gt box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Convert class label to one hot\n",
    "```python\n",
    "cls_num = 8\n",
    "batch_labels = [0, 4, 1, 7]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_num = 8\n",
    "batch_labels = [0, 4, 1, 7]\n",
    "\n",
    "def onehot_encode(cls_num, labels):\n",
    "    \"\"\" Encode batch labels to one hot representation. \n",
    "    \"\"\"\n",
    "    labels = torch.LongTensor(labels).reshape(-1,1)\n",
    "    onehot = torch.zeros(labels.size()[0], cls_num).scatter_(1, labels, 1)\n",
    "    return onehot\n",
    "\n",
    "onehot_encode(cls_num, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
