{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 Pytorch Object Detection Exercises\n",
    "\n",
    "The goal of this collection is to offer a quick reference for both old and new users who are designing and prototyping object detection algorithms with pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate coordinates of anchor boxes\n",
    "```python\n",
    "base_size = 16 # pixel size in original image, also named stride or downsampling factor. \n",
    "scale_ls = [8, 16, 32] # under the same aspect ratio, scale = w_a/w_b or h_a/h_b\n",
    "ratio_ls = [0.5, 1, 2] # under the same scale, ratio = height/width\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -84.,  -40.,   99.,   55.],\n",
      "        [ -56.,  -56.,   71.,   71.],\n",
      "        [ -36.,  -80.,   51.,   95.],\n",
      "        [-176.,  -88.,  191.,  103.],\n",
      "        [-120., -120.,  135.,  135.],\n",
      "        [ -80., -168.,   95.,  183.],\n",
      "        [-360., -184.,  375.,  199.],\n",
      "        [-248., -248.,  263.,  263.],\n",
      "        [-168., -344.,  183.,  359.]])\n"
     ]
    }
   ],
   "source": [
    "# code reference: https://github.com/NVIDIA/retinanet-examples/blob/c1ba8e7a9ffa036a387387426682e1a7b58ec707/retinanet/box.py#L5\n",
    "\n",
    "scale_ls = [8,16,32]\n",
    "ratio_ls = [0.5,1,2] \n",
    "base_size = 16\n",
    "\n",
    "scales = torch.FloatTensor(scale_ls).repeat(len(ratio_ls), 1) # shape: (len(ratio_ls), len(scale_ls)), (3,3)\n",
    "scales = scales.transpose(0, 1).contiguous().view(-1, 1) # shape: (len(ratio_ls)*len(scale_ls), 1), (9,1)\n",
    "ratios = torch.FloatTensor(ratio_ls).repeat(len(scale_ls)) # shape: (len(ratio_ls)*len(scale_ls),), (9,)\n",
    "\n",
    "base_wh = torch.FloatTensor([base_size]).repeat(len(ratios), 2) # base width and height, shape: (9,2)\n",
    "anchor_w = torch.round(torch.sqrt(base_wh[:, 0] * base_wh[:, 1] / ratios)) # anchor width, shape: (9,)\n",
    "anchor_h = torch.round(anchor_w * ratios) # anchor height, shape: (9,)\n",
    "anchor_wh = torch.stack((anchor_w, torch.round(anchor_w * ratios)), dim=1) # anchor width and height, shape: (9,2)\n",
    "# calculate the relative center coordinates, then the diff is the xmin,ymin and xmax,ymax\n",
    "xy_min = 0.5 * (base_wh - anchor_wh * scales) # 0.5*(base_wh-1) - 0.5*(anchor_wh-1)\n",
    "xy_max = 0.5 * (base_wh + anchor_wh * scales) - 1 # 0.5*(base_wh-1) + 0.5*(anchor_wh-1)\n",
    "anchor_coord = torch.cat([xy_min, xy_max], dim=1)\n",
    "print(anchor_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
